---
title: "Building RAG systems in Go with Ent, Atlas, and pgvector"
author: Rotem Tamir
authorURL: "https://github.com/rotemtam"
authorImageURL: "https://s.gravatar.com/avatar/36b3739951a27d2e37251867b7d44b1a?s=80"
authorTwitter: _rtam
image: "https://atlasgo.io/uploads/entrag.png"
---

本篇博文将探讨如何利用[Ent](https://entgo.io)、[Atlas](https://atlasgo.io)和[pgvector](https://github.com/pgvector/pgvector)构建[RAG](https://en.wikipedia.org/wiki/Retrieval-augmented_generation)（检索增强生成）系统。

RAG是一种通过引入检索步骤来增强生成模型能力的技术。不同于仅依赖模型内部知识，我们可以从外部数据源检索相关文档或数据，利用这些信息生成更准确、更具上下文感知的响应。这种方法特别适用于构建问答系统、聊天机器人等需要最新或领域特定知识的应用场景。

### 配置Ent数据模型

首先初始化项目Go模块：

```bash
go mod init github.com/rotemtam/entrag # Feel free to replace the module path with your own
```

本项目将使用Go实体框架[Ent](/)定义数据库结构。数据库用于存储待检索文档（按固定大小分块）及每个文本块的向量表示。运行以下命令初始化Ent项目：

```bash
go run -mod=mod entgo.io/ent/cmd/ent new Embedding Chunk
```

该命令会创建数据模型的占位文件，项目结构如下：

```
├── ent
│   ├── generate.go
│   └── schema
│       ├── chunk.go
│       └── embedding.go
├── go.mod
└── go.sum
```

接着定义`Chunk`模型结构。打开`ent/schema/chunk.go`文件进行如下定义：

```go title="ent/schema/chunk.go"
package schema

import (
	"entgo.io/ent"
	"entgo.io/ent/schema/edge"
	"entgo.io/ent/schema/field"
)

// Chunk holds the schema definition for the Chunk entity.
type Chunk struct {
	ent.Schema
}

// Fields of the Chunk.
func (Chunk) Fields() []ent.Field {
	return []ent.Field{
		field.String("path"),
		field.Int("nchunk"),
		field.Text("data"),
	}
}

// Edges of the Chunk.
func (Chunk) Edges() []ent.Edge {
	return []ent.Edge{
		edge.To("embedding", Embedding.Type).StorageKey(edge.Column("chunk_id")).Unique(),
	}
}
```

该模型包含三个字段：`path`存储文档路径、`nchunk`存储分块序号、`data`存储分块文本数据。同时定义了指向`Embedding`实体的边关系，用于存储文本块的向量表示。

继续之前，需安装`pgvector`扩展包。该PostgreSQL扩展提供向量运算和相似性搜索支持，用于存储和检索文本块的向量表示。

```bash
go get github.com/pgvector/pgvector-go
```

接下来定义`Embedding`模型。打开`ent/schema/embedding.go`文件进行如下定义：

```go title="ent/schema/embedding.go"
package schema

import (
	"entgo.io/ent"
	"entgo.io/ent/dialect"
	"entgo.io/ent/dialect/entsql"
	"entgo.io/ent/schema/edge"
	"entgo.io/ent/schema/field"
	"entgo.io/ent/schema/index"
	"github.com/pgvector/pgvector-go"
)

// Embedding holds the schema definition for the Embedding entity.
type Embedding struct {
	ent.Schema
}

// Fields of the Embedding.
func (Embedding) Fields() []ent.Field {
	return []ent.Field{
		field.Other("embedding", pgvector.Vector{}).
			SchemaType(map[string]string{
				dialect.Postgres: "vector(1536)",
			}),
	}
}

// Edges of the Embedding.
func (Embedding) Edges() []ent.Edge {
	return []ent.Edge{
		edge.From("chunk", Chunk.Type).Ref("embedding").Unique().Required(),
	}
}

func (Embedding) Indexes() []ent.Index {
	return []ent.Index{
		index.Fields("embedding").
			Annotations(
				entsql.IndexType("hnsw"),
				entsql.OpClass("vector_l2_ops"),
			),
	}
}
```

该模型包含`pgvector.Vector`类型的`embedding`字段存储向量数据。定义了指向`Chunk`实体的边关系，并使用`hnsw`索引类型和`vector_l2_ops`操作符类为`embedding`字段创建索引，该索引将支持高效的向量相似性搜索。

最后运行以下命令生成Ent代码：

```bash
go mod tidy
go generate ./...
```

Ent将根据模型定义生成相应的代码文件。

### 数据库配置

接下来配置PostgreSQL数据库。我们将使用Docker运行本地实例，由于需要`pgvector`扩展，选择预装该扩展的`pgvector/pgvector:pg17`镜像。

```bash
docker run --rm --name postgres -e POSTGRES_PASSWORD=pass -p 5432:5432 -d pgvector/pgvector:pg17
```

我们将使用与Ent集成的数据库Schema-as-Code工具[Atlas](https://atlasgo.io)来管理数据库结构。运行以下命令安装Atlas：

```
curl -sSfL https://atlasgo.io/install.sh | sh
```

其他安装方式请参阅[Atlas安装文档](https://atlasgo.io/getting-started#installation)。

由于需要管理扩展，需注册Atlas Pro账户。可通过以下命令获取免费试用：

```
atlas login
```

:::note[不使用迁移工具的情况]

若希望跳过Atlas的使用，您可以直接通过[此文件](https://github.com/rotemtam/entrag/blob/e91722c0fbe011b03dbd6b9e68415547c8b7bba4/setup.sql#L1)中的SQL语句将所需模式应用到数据库。

:::

现在，让我们创建Atlas配置，将`base.pg.hcl`文件与Ent模式组合：

```hcl title="atlas.hcl"
data "composite_schema" "schema" {
  schema {
    url = "file://base.pg.hcl"
  }
  schema "public" {
    url = "ent://ent/schema"
  }
}

env "local" {
  url = getenv("DB_URL")
  schema {
    src = data.composite_schema.schema.url
  }
  dev = "docker://pgvector/pg17/dev"
}
```

该配置定义了一个复合模式，包含`base.pg.hcl`文件和Ent模式。我们还定义了一个名为`local`的环境，使用该复合模式进行本地开发。`dev`字段指定了[开发数据库](https://atlasgo.io/concepts/dev-database)的URL，Atlas利用此URL来规范化模式并进行各种计算。

接下来，通过运行以下命令将模式应用到数据库：

```bash
export DB_URL='postgresql://postgres:pass@localhost:5432/postgres?sslmode=disable'
atlas schema apply --env local
```

Atlas将从我们的配置加载数据库的期望状态，将其与数据库当前状态进行比较，并创建一个迁移计划以使数据库达到期望状态：

```
Planning migration statements (5 in total):

  -- create extension "vector":
    -> CREATE EXTENSION "vector" WITH SCHEMA "public" VERSION "0.8.0";
  -- create "chunks" table:
    -> CREATE TABLE "public"."chunks" (
         "id" bigint NOT NULL GENERATED BY DEFAULT AS IDENTITY,
         "path" character varying NOT NULL,
         "nchunk" bigint NOT NULL,
         "data" text NOT NULL,
         PRIMARY KEY ("id")
       );
  -- create "embeddings" table:
    -> CREATE TABLE "public"."embeddings" (
         "id" bigint NOT NULL GENERATED BY DEFAULT AS IDENTITY,
         "embedding" public.vector(1536) NOT NULL,
         "chunk_id" bigint NOT NULL,
         PRIMARY KEY ("id"),
         CONSTRAINT "embeddings_chunks_embedding" FOREIGN KEY ("chunk_id") REFERENCES "public"."chunks" ("id") ON UPDATE NO ACTION ON DELETE NO ACTION
       );
  -- create index "embedding_embedding" to table: "embeddings":
    -> CREATE INDEX "embedding_embedding" ON "public"."embeddings" USING hnsw ("embedding" vector_l2_ops);
  -- create index "embeddings_chunk_id_key" to table: "embeddings":
    -> CREATE UNIQUE INDEX "embeddings_chunk_id_key" ON "public"."embeddings" ("chunk_id");

-------------------------------------------

Analyzing planned statements (5 in total):

  -- non-optimal columns alignment:
    -- L4: Table "chunks" has 8 redundant bytes of padding per row. To reduce disk space,
       the optimal order of the columns is as follows: "id", "nchunk", "path",
       "data" https://atlasgo.io/lint/analyzers#PG110
  -- ok (370.25µs)

  -------------------------
  -- 114.306667ms
  -- 5 schema changes
  -- 1 diagnostic

-------------------------------------------

? Approve or abort the plan:
  ▸ Approve and apply
    Abort
```

除了规划变更外，Atlas还会提供诊断和建议以优化模式。在本例中，它建议重新排序`chunks`表中的列以减少磁盘空间。由于本教程不关注磁盘空间，我们可以选择`Approve and apply`继续迁移。

最后，为了验证模式是否成功应用，我们可以重新运行`atlas schema apply`命令。Atlas将输出：

```bash
Schema is synced, no changes to be made
```

### 搭建CLI框架

现在我们的数据库模式已设置完毕，让我们搭建CLI应用程序框架。本教程将使用[`alecthomas/kong`](https://github.com/alecthomas/kong)库构建一个小型应用，用于加载、索引和查询数据库中的文档。

首先，安装`kong`库：

```bash
go get github.com/alecthomas/kong
```

接下来，创建一个名为`cmd/entrag/main.go`的新文件，并按如下方式定义CLI应用程序：

```go title="cmd/entrag/main.go"
package main

import (
	"fmt"
	"os"

	"github.com/alecthomas/kong"
)

// CLI holds global options and subcommands.
type CLI struct {
	// DBURL is read from the environment variable DB_URL.
	DBURL     string `kong:"env='DB_URL',help='Database URL for the application.'"`
	OpenAIKey string `kong:"env='OPENAI_KEY',help='OpenAI API key for the application.'"`

	// Subcommands
	Load  *LoadCmd  `kong:"cmd,help='Load command that accepts a path.'"`
	Index *IndexCmd `kong:"cmd,help='Create embeddings for any chunks that do not have one.'"`
	Ask   *AskCmd   `kong:"cmd,help='Ask a question about the indexed documents'"`
}

func main() {
	var cli CLI
	app := kong.Parse(&cli,
		kong.Name("entrag"),
		kong.Description("Ask questions about markdown files."),
		kong.UsageOnError(),
	)
	if err := app.Run(&cli); err != nil {
		fmt.Fprintf(os.Stderr, "Error: %s\n", err)
		os.Exit(1)
	}
}
```

再创建一个名为`cmd/entrag/rag.go`的文件，内容如下：

```go title="cmd/entrag/rag.go"
package main

type (
	// LoadCmd loads the markdown files into the database.
	LoadCmd struct {
		Path string `help:"path to dir with markdown files" type:"existingdir" required:""`
	}
	// IndexCmd creates the embedding index on the database.
	IndexCmd struct {
	}
	// AskCmd is another leaf command.
	AskCmd struct {
		// Text is the positional argument for the ask command.
		Text string `kong:"arg,required,help='Text for the ask command.'"`
	}
)
```

通过运行以下命令验证我们搭建的CLI应用程序是否正常工作：

```bash
go run ./cmd/entrag --help
```

如果一切设置正确，您应该能看到CLI应用程序的帮助输出：

```
Usage: entrag <command> [flags]

Ask questions about markdown files.

Flags:
  -h, --help                  Show context-sensitive help.
      --dburl=STRING          Database URL for the application ($DB_URL).
      --open-ai-key=STRING    OpenAI API key for the application ($OPENAI_KEY).

Commands:
  load --path=STRING [flags]
    Load command that accepts a path.

  index [flags]
    Create embeddings for any chunks that do not have one.

  ask <text> [flags]
    Ask a question about the indexed documents

Run "entrag <command> --help" for more information on a command.
```

### 将文档加载到数据库

接下来，我们需要一些Markdown文件加载到数据库中。创建一个名为`data`的目录，并向其中添加一些Markdown文件。在本例中，我下载了[`ent/ent`](https://github.com/ent/ent)仓库，并使用其`docs`目录作为Markdown文件的来源。

现在，让我们实现`LoadCmd`命令以将Markdown文件加载到数据库。打开`cmd/entrag/rag.go`文件并添加以下代码：

```go title="cmd/entrag/rag.go"
const (
	tokenEncoding = "cl100k_base"
	chunkSize     = 1000
)

// Run is the method called when the "load" command is executed.
func (cmd *LoadCmd) Run(ctx *CLI) error {
	client, err := ctx.entClient()
	if err != nil {
		return fmt.Errorf("failed opening connection to postgres: %w", err)
	}
	tokTotal := 0
	return filepath.WalkDir(ctx.Load.Path, func(path string, d fs.DirEntry, err error) error {
		if filepath.Ext(path) == ".mdx" || filepath.Ext(path) == ".md" {
			chunks := breakToChunks(path)
			for i, chunk := range chunks {
				tokTotal += len(chunk)
				client.Chunk.Create().
					SetData(chunk).
					SetPath(path).
					SetNchunk(i).
					SaveX(context.Background())
			}
		}
		return nil
	})
}

func (c *CLI) entClient() (*ent.Client, error) {
	return ent.Open("postgres", c.DBURL)
}
```

此代码定义了`LoadCmd`命令的`Run`方法。该方法从指定路径读取Markdown文件，将其拆分为每个1000个token的块，并保存到数据库。我们使用`entClient`方法创建一个新的Ent客户端，该客户端使用CLI选项中指定的数据库URL。

关于`breakToChunks`的实现，请参考[完整代码](https://github.com/rotemtam/entrag/blob/93291e0c8479ecabd5f2a2e49fbaa8c49f995e70/cmd/entrag/rag.go#L157)，该代码位于[`entrag`仓库](https://github.com/rotemtam/entrag)中，其实现几乎完全基于[Eli Bendersky关于Go中RAG的入门指南](https://eli.thegreenplace.net/2023/retrieval-augmented-generation-in-go/)。

最后，运行`load`命令将Markdown文件加载到数据库中：

```bash
go run ./cmd/entrag load --path=data
```

命令执行完成后，您应该会看到数据块已成功加载到数据库中。可通过以下命令验证：

```bash
docker exec -it postgres psql -U postgres -d postgres -c "SELECT COUNT(*) FROM chunks;"
```

您将看到类似以下输出：

```
  count
-------
   276
(1 row)
```

### 索引嵌入向量

现在文档已加载到数据库，我们需要为每个数据块生成嵌入向量。我们将使用OpenAI API来生成这些嵌入向量。为此，需要先安装`openai`包：

```bash
go get github.com/sashabaranov/go-openai
```

如果您还没有OpenAI API密钥，可以在[OpenAI平台](https://platform.openai.com/signup)注册账号并[生成API密钥](https://platform.openai.com/api-keys)。

我们将从环境变量`OPENAI_KEY`中读取该密钥，请先设置该变量：

```bash
export OPENAI_KEY=<your OpenAI API key>
```

接下来，实现`IndexCmd`命令来为数据块创建嵌入向量。打开`cmd/entrag/rag.go`文件并添加以下代码：

```go title="cmd/entrag/rag.go"
// Run is the method called when the "index" command is executed.
func (cmd *IndexCmd) Run(cli *CLI) error {
	client, err := cli.entClient()
	if err != nil {
		return fmt.Errorf("failed opening connection to postgres: %w", err)
	}
	ctx := context.Background()
	chunks := client.Chunk.Query().
		Where(
			chunk.Not(
				chunk.HasEmbedding(),
			),
		).
		Order(ent.Asc(chunk.FieldID)).
		AllX(ctx)
	for _, ch := range chunks {
		log.Println("Created embedding for chunk", ch.Path, ch.Nchunk)
		embedding := getEmbedding(ch.Data)
		_, err := client.Embedding.Create().
			SetEmbedding(pgvector.NewVector(embedding)).
			SetChunk(ch).
			Save(ctx)
		if err != nil {
			return fmt.Errorf("error creating embedding: %v", err)
		}
	}
	return nil
}

// getEmbedding invokes the OpenAI embedding API to calculate the embedding
// for the given string. It returns the embedding.
func getEmbedding(data string) []float32 {
	client := openai.NewClient(os.Getenv("OPENAI_KEY"))
	queryReq := openai.EmbeddingRequest{
		Input: []string{data},
		Model: openai.AdaEmbeddingV2,
	}
	queryResponse, err := client.CreateEmbeddings(context.Background(), queryReq)
	if err != nil {
		log.Fatalf("Error getting embedding: %v", err)
	}
	return queryResponse.Data[0].Embedding
}
```

我们已为`IndexCmd`命令定义了`Run`方法。该方法会查询数据库中尚未生成嵌入向量的数据块，使用OpenAI API为每个数据块生成嵌入向量，并将这些嵌入向量保存回数据库。

最后，运行`index`命令为数据块创建嵌入向量：

```bash
go run ./cmd/entrag index
```

您将看到类似以下日志：

```
2025/02/13 13:04:42 Created embedding for chunk /Users/home/entr/data/md/aggregate.md 0
2025/02/13 13:04:43 Created embedding for chunk /Users/home/entr/data/md/ci.mdx 0
2025/02/13 13:04:44 Created embedding for chunk /Users/home/entr/data/md/ci.mdx 1
2025/02/13 13:04:45 Created embedding for chunk /Users/home/entr/data/md/ci.mdx 2
2025/02/13 13:04:46 Created embedding for chunk /Users/home/entr/data/md/code-gen.md 0
2025/02/13 13:04:47 Created embedding for chunk /Users/home/entr/data/md/code-gen.md 1
```

### 提问功能

现在文档已加载且嵌入向量已生成，我们可以实现`AskCmd`命令来对索引文档进行提问。打开`cmd/entrag/rag.go`文件并添加以下代码：

```go title="cmd/entrag/rag.go"
// Run is the method called when the "ask" command is executed.
func (cmd *AskCmd) Run(ctx *CLI) error {
	client, err := ctx.entClient()
	if err != nil {
		return fmt.Errorf("failed opening connection to postgres: %w", err)
	}
	question := cmd.Text
	emb := getEmbedding(question)
	embVec := pgvector.NewVector(emb)
	embs := client.Embedding.
		Query().
		Order(func(s *sql.Selector) {
			s.OrderExpr(sql.ExprP("embedding <-> $1", embVec))
		}).
		WithChunk().
		Limit(5).
		AllX(context.Background())
	b := strings.Builder{}
	for _, e := range embs {
		chnk := e.Edges.Chunk
		b.WriteString(fmt.Sprintf("From file: %v\n", chnk.Path))
		b.WriteString(chnk.Data)
	}
	query := fmt.Sprintf(`Use the below information from the ent docs to answer the subsequent question.
Information:
%v

Question: %v`, b.String(), question)
	oac := openai.NewClient(ctx.OpenAIKey)
	resp, err := oac.CreateChatCompletion(
		context.Background(),
		openai.ChatCompletionRequest{
			Model: openai.GPT4o,
			Messages: []openai.ChatCompletionMessage{

				{
					Role:    openai.ChatMessageRoleUser,
					Content: query,
				},
			},
		},
	)
	if err != nil {
		return fmt.Errorf("error creating chat completion: %v", err)
	}
	choice := resp.Choices[0]
	out, err := glamour.Render(choice.Message.Content, "dark")
	fmt.Print(out)
	return nil
}
```

至此，所有组件都已准备就绪。在数据库完成文档及其嵌入向量的准备工作后，我们现在可以针对这些文档提问了。让我们分解`AskCmd`命令的实现：

```go
emb := getEmbedding(question)
embVec := pgvector.NewVector(emb)
embs := client.Embedding.
    Query().
    Order(func(s *sql.Selector) {
        s.OrderExpr(sql.ExprP("embedding <-> $1", embVec))
    }).
    WithChunk().
    Limit(5).
    AllX(context.Background())
```

首先，我们使用OpenAI API将用户的问题转换为向量。利用该向量，我们希望找到数据库中与之最相似的嵌入向量。我们查询数据库中的嵌入向量，使用`pgvector`的`<->`运算符按相似度排序，并将结果限制为前5个。

```go
for _, e := range embs {
		chnk := e.Edges.Chunk
		b.WriteString(fmt.Sprintf("From file: %v\n", chnk.Path))
		b.WriteString(chnk.Data)
	}
	query := fmt.Sprintf(`Use the below information from the ent docs to answer the subsequent question.
Information:
%v

Question: %v`, b.String(), question)
```

接着，我们准备将前5个数据块的信息作为问题的上下文。然后将问题和上下文格式化为单个字符串。

```go
oac := openai.NewClient(ctx.OpenAIKey)
resp, err := oac.CreateChatCompletion(
    context.Background(),
    openai.ChatCompletionRequest{
        Model: openai.GPT4o,
        Messages: []openai.ChatCompletionMessage{

            {
                Role:    openai.ChatMessageRoleUser,
                Content: query,
            },
        },
    },
)
if err != nil {
    return fmt.Errorf("error creating chat completion: %v", err)
}
choice := resp.Choices[0]
out, err := glamour.Render(choice.Message.Content, "dark")
fmt.Print(out)
```

然后，我们使用OpenAI API生成问题的回答。我们将问题和上下文传递给API并接收响应。最后使用`glamour`包渲染响应，以便在终端中显示。

在运行`ask`命令之前，请先安装`glamour`包：

```bash
go get github.com/charmbracelet/glamour
```

最后，运行 `ask` 命令向已索引的文档提问：

```bash
go run ./cmd/entrag ask "tl;dr What is Ent?"
```

我们的RAG系统会给出如下响应：

```
  Ent is an open-source entity framework (ORM) for the Go programming language. It
  allows developers to define data models or graph-structures in Go code. Ent
  emphasizes principles such as schema as code, a statically typed and explicit
  API generated through codegen, simple queries and graph traversals, statically
  typed predicates, and storage agnosticism. It supports various databases,
  including MySQL, MariaDB, PostgreSQL, SQLite, and Gremlin-based graph databases,
  and aims to enhance productivity in Go development.
```

太棒了！我们成功使用Ent、Atlas和pgvector构建了一个RAG系统。现在可以向数据库中加载的文档提问，并获取结合上下文的智能回答。

以下是我们这个精巧RAG系统的更多问答示例：

<details>
<summary>go run ./cmd/entrag ask "how to define order field in entgql"</summary>
 To define an order field in  entgql , you need to annotate a field in your  Ent
  schema using the  entgql.Annotation  with the  OrderField . Here's a step-by-
  step
  guide on how you can do it:

  1. Choose a Comparable Field: Select the field in your schema that you want to
  be orderable. This could be a text field, a timestamp, an integer, an enum, etc.
  2. Annotate the Field: Use the  entgql.OrderField  annotation on the chosen
  field. The order field name should be uppercase and match its corresponding enum
  value in the GraphQL schema.
  3. Update your Schema: Here’s an example of how to annotate your fields in the
  ent/schema :
    ```go
    func (Todo) Fields() []ent.Field {
        return []ent.Field{
            field.Text("text").
                NotEmpty().
                Annotations(
                    entgql.OrderField("TEXT"),
                ),
            field.Time("created_at").
                Default(time.Now).
                Immutable().
                Annotations(
                    entgql.OrderField("CREATED_AT"),
                ),
            field.Enum("status").
                NamedValues(
                    "InProgress", "IN_PROGRESS",
                    "Completed", "COMPLETED",
                ).
                Default("IN_PROGRESS").
                Annotations(
                    entgql.OrderField("STATUS"),
                ),
            field.Int("priority").
                Default(0).
                Annotations(
                    entgql.OrderField("PRIORITY"),
                ),
        }
    }
    ```
  4. Alternatively, Multiple Order Fields: If you want to enable ordering by
  multiple fields, you can use the  entgql.MultiOrder()  annotation:
    ```go
    func (Todo) Annotations() []schema.Annotation {
        return []schema.Annotation{
            entgql.MultiOrder(),
        }
    }
    ```

  5. Generate GraphQL Types: Ensure that the GraphQL types match this setup in
  your GraphQL schema, for example:
    ```graphql
    enum OrderDirection {
      ASC
      DESC
    }
    enum TodoOrderField {
      CREATED_AT
      PRIORITY
      STATUS
      TEXT
    }
    input TodoOrder {
      direction: OrderDirection!
      field: TodoOrderField
    }
    ```

  6. Add  orderBy  Argument to Queries: Include the  orderBy  parameter in your
  GraphQL queries to allow clients to sort items:
    ```graphql
    type Query {
      todos(
        after: Cursor
        first: Int
        before: Cursor
        last: Int
        orderBy: TodoOrder
      ): TodoConnection!
    }
    ```

  By following these steps, you ensure that your Ent-based application can support
  sorting in your GraphQL API based on various fields.


</details>

<details>
<summary> go run ./cmd/entrag ask "what's the difference between privacy rules and interceptors" </summary>
 Privacy rules and interceptors serve different purposes within the Ent
  framework, each providing distinct functionalities:

1. Privacy Rules:
* Purpose: Privacy rules are primarily concerned with enforcing access
control policies for data queries and mutations. They determine whether an
operation is allowed or denied based on predefined rules.
* Implementation: They are implemented as part of a privacy policy using the
ent.Policy  interface which includes  EvalQuery  and  EvalMutation  methods.
* Operation: Privacy rules evaluate whether specific conditions are met
before allowing or denying access to the data. They can return decisions
like  privacy.Allow ,  privacy.Deny , or  privacy.Skip  to control the flow
of evaluation.
* Use Case: Ideal for managing access control by ensuring that users can
only perform certain operations if they meet the specified criteria.
2. Interceptors:
* Purpose: Interceptors act as middleware for Ent queries, allowing
modification and customization of query behaviors. They can be used to
augment or modify queries during different stages of their lifecycle.
* Implementation: Implemented as interfaces or using the  ent.InterceptFunc
adapter. They intercept and potentially modify queries by working on the
read-path.
* Operation: Interceptors modify or enhance queries, typically without the
access control logic inherent in privacy rules. They provide hooks to
execute custom logic pre and post query execution.
* Use Case: Suitable for generic transformations or modifications to queries,
such as adding default filters, query limitations, or logging operations
without focusing on access control.


In summary, while privacy rules focus on access control, interceptors are about
managing and modifying the query execution process.
</details>

### 总结

本篇博文中，我们探索了如何使用Ent、Atlas和pgvector构建RAG系统。特别感谢[Eli Bendersky](https://eli.thegreenplace.net/2023/retrieval-augmented-generation-in-go/)的精彩博文以及他多年来出色的Go技术写作！